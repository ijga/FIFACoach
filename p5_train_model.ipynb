{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from p4_save_pytorch_datasets import FIFAGSDataset \n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: FIFAGSDataset(568):\n",
      "====================\n",
      "Number of graphs: 568\n",
      "Number of features: 3\n",
      "Number of classes: 9\n",
      "\n",
      "Data(x=[19, 3], edge_index=[2, 89], edge_attr=[89, 3], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 19\n",
      "Number of edges: 89\n",
      "Average node degree: 4.68\n",
      "Has isolated nodes: True\n",
      "Has self-loops: False\n",
      "Is undirected: False\n",
      "Number of training graphs: 475\n",
      "Number of test graphs: 93\n",
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[600, 3], edge_index=[2, 3396], edge_attr=[3396, 3], y=[32], batch=[600], ptr=[33])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[615, 3], edge_index=[2, 3456], edge_attr=[3456, 3], y=[32], batch=[615], ptr=[33])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[603, 3], edge_index=[2, 3281], edge_attr=[3281, 3], y=[32], batch=[603], ptr=[33])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[611, 3], edge_index=[2, 3124], edge_attr=[3124, 3], y=[32], batch=[611], ptr=[33])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[634, 3], edge_index=[2, 3695], edge_attr=[3695, 3], y=[32], batch=[634], ptr=[33])\n",
      "\n",
      "Step 6:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[608, 3], edge_index=[2, 3529], edge_attr=[3529, 3], y=[32], batch=[608], ptr=[33])\n",
      "\n",
      "Step 7:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[595, 3], edge_index=[2, 3306], edge_attr=[3306, 3], y=[32], batch=[595], ptr=[33])\n",
      "\n",
      "Step 8:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[596, 3], edge_index=[2, 3437], edge_attr=[3437, 3], y=[32], batch=[596], ptr=[33])\n",
      "\n",
      "Step 9:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[603, 3], edge_index=[2, 3413], edge_attr=[3413, 3], y=[32], batch=[603], ptr=[33])\n",
      "\n",
      "Step 10:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[582, 3], edge_index=[2, 3120], edge_attr=[3120, 3], y=[32], batch=[582], ptr=[33])\n",
      "\n",
      "Step 11:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[595, 3], edge_index=[2, 3362], edge_attr=[3362, 3], y=[32], batch=[595], ptr=[33])\n",
      "\n",
      "Step 12:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[616, 3], edge_index=[2, 3402], edge_attr=[3402, 3], y=[32], batch=[616], ptr=[33])\n",
      "\n",
      "Step 13:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[594, 3], edge_index=[2, 3390], edge_attr=[3390, 3], y=[32], batch=[594], ptr=[33])\n",
      "\n",
      "Step 14:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[626, 3], edge_index=[2, 3548], edge_attr=[3548, 3], y=[32], batch=[626], ptr=[33])\n",
      "\n",
      "Step 15:\n",
      "=======\n",
      "Number of graphs in the current batch: 27\n",
      "DataBatch(x=[517, 3], edge_index=[2, 2784], edge_attr=[2784, 3], y=[27], batch=[517], ptr=[28])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ROOT = '.'\n",
    "\n",
    "dataset = FIFAGSDataset(ROOT)\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[1]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "torch.manual_seed(153)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:475]\n",
    "test_dataset = dataset[475:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "# # Usage\n",
    "# dataset = MyDataset(root='path/to/dataset')\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, num_node_features, hidden_channels):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.lin = torch.nn.Linear(hidden_channels, 1)  # Binary classification\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# model = GCN(num_node_features=1, hidden_channels=16)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# def train():\n",
    "#     model.train()\n",
    "#     for data in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(data)\n",
    "#         loss = F.binary_cross_entropy_with_logits(out, data.y.float().unsqueeze(1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# for epoch in range(1, 201):\n",
    "#     train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.5979, Test Acc: 0.6237\n",
      "Epoch: 002, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 003, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 004, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 005, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 006, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 007, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 008, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 009, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 010, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 011, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 012, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 013, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 014, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 015, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 016, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 017, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 018, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 019, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 020, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 021, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 022, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 023, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 024, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 025, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 026, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 027, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 028, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 029, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 030, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 031, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 032, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 033, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 034, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 035, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 036, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 037, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 038, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 039, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 040, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 041, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 042, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 043, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 044, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 045, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 046, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 047, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 048, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 049, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 050, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 051, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 052, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 053, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 054, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 055, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 056, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 057, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 058, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 059, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 060, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 061, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 062, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 063, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 064, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 065, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 066, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 067, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 068, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 069, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 070, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 071, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 072, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 073, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 074, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 075, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 076, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 077, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 078, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 079, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 080, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 081, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 082, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 083, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 084, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 085, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 086, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 087, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 088, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 089, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 090, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 091, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 092, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 093, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 094, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 095, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 096, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 097, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 098, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 099, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 100, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 101, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 102, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 103, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 104, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 105, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 106, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 107, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 108, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 109, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 110, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 111, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 112, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 113, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 114, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 115, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 116, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 117, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 118, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 119, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 120, Train Acc: 0.6000, Test Acc: 0.6237\n",
      "Epoch: 121, Train Acc: 0.6000, Test Acc: 0.6237\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset)  \u001b[38;5;66;03m# Derive ratio of correct predictions.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m171\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m test(train_loader)\n\u001b[1;32m     32\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(test_loader)\n",
      "Cell \u001b[0;32mIn[40], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)  \u001b[38;5;66;03m# Compute the loss.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Derive gradients.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update parameters based on gradients.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
